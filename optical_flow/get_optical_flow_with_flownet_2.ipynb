{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tDQdXaknUfl"
      },
      "source": [
        "To use this, you need to download the desired flownet pre-trained model weights(flownet2 for this project) from https://github.com/NVIDIA/flownet2-pytorch and keep it in a google drive folder. \n",
        "\n",
        "You can also download it directly in colab, or on your system, but the flownet model with pretrained weights are transferred by google drive file share which might make this harder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw_f76igD0Rn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "005fb982-5ed4-4350-c614-6ef728e01ee0"
      },
      "source": [
        "#pull github code\n",
        "!git clone https://github.com/NVIDIA/flownet2-pytorch.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'flownet2-pytorch'...\n",
            "remote: Enumerating objects: 557, done.\u001b[K\n",
            "remote: Total 557 (delta 0), reused 0 (delta 0), pack-reused 557\u001b[K\n",
            "Receiving objects: 100% (557/557), 6.28 MiB | 19.67 MiB/s, done.\n",
            "Resolving deltas: 100% (312/312), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTY98g4S7H5d"
      },
      "source": [
        "After cloning the repo, for colab, you need to perform these changes \n",
        "\n",
        "In setup.py in /content/flownet2-pytorch/networks/(channelnorm_package, correlation_package, and resample_2d_package) \n",
        "1. change cxx_args = ['-std=c++11'] to cxx_args = ['-std=c++14'] \n",
        "2. Add '-gencode', 'arch=compute_37,code=sm_37' to nvcc-args \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9yZeFCl7dIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab3864d-053c-40c0-defb-07294bb505d8"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec 20 20:54:02 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P0    26W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKyDhPgj95Fs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d32f555e-77eb-4363-e215-4e653dbccdea"
      },
      "source": [
        "#install tensorboardX, setproctitle, colorama\n",
        "!pip install tensorboardX\n",
        "!pip install setproctitle\n",
        "!pip install colorama"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 12.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (3.19.6)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Installing collected packages: setproctitle\n",
            "Successfully installed setproctitle-1.3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkUa7LwZe3ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8376e00b-0fb6-4a59-fc0d-bca38b7c4a9f"
      },
      "source": [
        "!pip install scipy==1.1.0\n",
        "!pip install Pillow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scipy==1.1.0\n",
            "  Downloading scipy-1.1.0.tar.gz (15.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.6 MB 15.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: scipy\n",
            "  Building wheel for scipy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scipy: filename=scipy-1.1.0-cp38-cp38-linux_x86_64.whl size=41840179 sha256=ac80c73e13263764d58a060b7d41f747206b9df62dba9d1b1679625e4c26a01d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/a9/5e/9e4eddde37a1e15cf5cb404ba197df482cc39ffbfef91ec337\n",
            "Successfully built scipy\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.4.0 requires scipy>=1.6, but you have scipy 1.1.0 which is incompatible.\n",
            "pymc 4.1.4 requires scipy>=1.4.1, but you have scipy 1.1.0 which is incompatible.\n",
            "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.1.0 which is incompatible.\n",
            "jaxlib 0.3.25+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.1.0 which is incompatible.\n",
            "jax 0.3.25 requires scipy>=1.5, but you have scipy 1.1.0 which is incompatible.\n",
            "aeppl 0.0.33 requires scipy>=1.4.0, but you have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "Successfully installed scipy-1.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIYVMe9xhRu1",
        "outputId": "3ff466f8-7d32-4378-b3f8-6f19e7552884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the cloned flownet2-pytorch (with the modified arguments) are being stored in the directory below\n",
        "%cd /content/drive/MyDrive/flownet2-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9hGJhHrjfhr",
        "outputId": "8c19a97d-cbe6-4eb6-8189-87279a5e3a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/flownet2-pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZaz2ZqliT1B"
      },
      "source": [
        "Then install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HxiF1ylMDNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b7dd20-341f-43c0-d9b6-51dc5e9db45b"
      },
      "source": [
        "!bash install.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating correlation_cuda.egg-info\n",
            "writing correlation_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to correlation_cuda.egg-info/dependency_links.txt\n",
            "writing top-level names to correlation_cuda.egg-info/top_level.txt\n",
            "writing manifest file 'correlation_cuda.egg-info/SOURCES.txt'\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "writing manifest file 'correlation_cuda.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:387: UserWarning: The detected CUDA version (11.2) has a minor version mismatch with the version that was used to compile PyTorch (11.6). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "building 'correlation_cuda' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.8\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c correlation_cuda.cc -o build/temp.linux-x86_64-3.8/correlation_cuda.o -std=c++14 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=correlation_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c correlation_cuda_kernel.cu -o build/temp.linux-x86_64-3.8/correlation_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_70,code=compute_70 -gencode arch=compute_37,code=sm_37 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=correlation_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input\u001b[01;35m\u001b[K1\u001b[m\u001b[K.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:166:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:977:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:1005:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:1855:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:1882:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:2737:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:2768:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input\u001b[01;35m\u001b[K2\u001b[m\u001b[K.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:166:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:977:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:1005:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:1855:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:1882:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:2737:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:2768:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input\u001b[01;35m\u001b[K1\u001b[m\u001b[K.type(), \"correlation_forward\", ([&] {\n",
            "                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:165:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:987:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:1059:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:1128:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:1997:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:2068:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:2136:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:3010:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:3085:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:3157:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(inp\u001b[01;35m\u001b[Ku\u001b[m\u001b[Kt1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:163:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:974:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:1002:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:1852:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:1879:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:2734:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:2765:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(inp\u001b[01;35m\u001b[Ku\u001b[m\u001b[Kt2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:163:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:974:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:1002:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:1852:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:1879:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:2734:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:2765:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(i\u001b[01;35m\u001b[Kn\u001b[m\u001b[Kput2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:163:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:1000:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:1072:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:1144:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:2028:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:2099:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:2170:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:3059:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:3134:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:3209:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:45:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rI\u001b[01;35m\u001b[Kn\u001b[m\u001b[Kput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:164:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:1001:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:1073:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:1145:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:2029:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:2100:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:2171:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:3060:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:3135:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:3210:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-3.8\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/correlation_cuda.o build/temp.linux-x86_64-3.8/correlation_cuda_kernel.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/correlation_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.8/correlation_cuda.cpython-38-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for correlation_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/correlation_cuda.py to correlation_cuda.cpython-38.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying correlation_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying correlation_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying correlation_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying correlation_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.correlation_cuda.cpython-38: module references __file__\n",
            "creating dist\n",
            "creating 'dist/correlation_cuda-0.0.0-py3.8-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing correlation_cuda-0.0.0-py3.8-linux-x86_64.egg\n",
            "creating /root/.local/lib/python3.8/site-packages/correlation_cuda-0.0.0-py3.8-linux-x86_64.egg\n",
            "Extracting correlation_cuda-0.0.0-py3.8-linux-x86_64.egg to /root/.local/lib/python3.8/site-packages\n",
            "Adding correlation-cuda 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /root/.local/lib/python3.8/site-packages/correlation_cuda-0.0.0-py3.8-linux-x86_64.egg\n",
            "Processing dependencies for correlation-cuda==0.0.0\n",
            "Finished processing dependencies for correlation-cuda==0.0.0\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating resample2d_cuda.egg-info\n",
            "writing resample2d_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to resample2d_cuda.egg-info/dependency_links.txt\n",
            "writing top-level names to resample2d_cuda.egg-info/top_level.txt\n",
            "writing manifest file 'resample2d_cuda.egg-info/SOURCES.txt'\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "writing manifest file 'resample2d_cuda.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:387: UserWarning: The detected CUDA version (11.2) has a minor version mismatch with the version that was used to compile PyTorch (11.6). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "building 'resample2d_cuda' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.8\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c resample2d_cuda.cc -o build/temp.linux-x86_64-3.8/resample2d_cuda.o -std=c++14 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=resample2d_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c resample2d_kernel.cu -o build/temp.linux-x86_64-3.8/resample2d_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_70,code=compute_70 -gencode arch=compute_37,code=sm_37 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=resample2d_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid resample2d_kernel_forward(at::Tensor&, at::Tensor&, at::Tensor&, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:221:173:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_update_output<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:221:225:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_update_output<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:221:277:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_update_output<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid resample2d_kernel_backward(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:269:175:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input1<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:269:227:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input1<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:269:283:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input1<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:269:347:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input1<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:298:175:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input2<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:298:227:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input2<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:298:283:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input2<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:298:347:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input2<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-3.8\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/resample2d_cuda.o build/temp.linux-x86_64-3.8/resample2d_kernel.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/resample2d_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.8/resample2d_cuda.cpython-38-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for resample2d_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/resample2d_cuda.py to resample2d_cuda.cpython-38.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying resample2d_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying resample2d_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying resample2d_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying resample2d_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.resample2d_cuda.cpython-38: module references __file__\n",
            "creating dist\n",
            "creating 'dist/resample2d_cuda-0.0.0-py3.8-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing resample2d_cuda-0.0.0-py3.8-linux-x86_64.egg\n",
            "creating /root/.local/lib/python3.8/site-packages/resample2d_cuda-0.0.0-py3.8-linux-x86_64.egg\n",
            "Extracting resample2d_cuda-0.0.0-py3.8-linux-x86_64.egg to /root/.local/lib/python3.8/site-packages\n",
            "Adding resample2d-cuda 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /root/.local/lib/python3.8/site-packages/resample2d_cuda-0.0.0-py3.8-linux-x86_64.egg\n",
            "Processing dependencies for resample2d-cuda==0.0.0\n",
            "Finished processing dependencies for resample2d-cuda==0.0.0\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating channelnorm_cuda.egg-info\n",
            "writing channelnorm_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to channelnorm_cuda.egg-info/dependency_links.txt\n",
            "writing top-level names to channelnorm_cuda.egg-info/top_level.txt\n",
            "writing manifest file 'channelnorm_cuda.egg-info/SOURCES.txt'\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "writing manifest file 'channelnorm_cuda.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:387: UserWarning: The detected CUDA version (11.2) has a minor version mismatch with the version that was used to compile PyTorch (11.6). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "building 'channelnorm_cuda' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.8\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c channelnorm_cuda.cc -o build/temp.linux-x86_64-3.8/channelnorm_cuda.o -std=c++14 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=channelnorm_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c channelnorm_kernel.cu -o build/temp.linux-x86_64-3.8/channelnorm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_70,code=compute_70 -gencode arch=compute_37,code=sm_37 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=channelnorm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(inp\u001b[01;35m\u001b[Ku\u001b[m\u001b[Kt1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:165:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:1020:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:1075:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:1956:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:2010:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:2896:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:2954:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(inp\u001b[01;35m\u001b[Ku\u001b[m\u001b[Kt1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:173:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1030:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1085:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1144:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1211:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:2102:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:2156:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:2214:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:2280:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:3176:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:3234:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:3296:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:3366:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:238:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-3.8\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/channelnorm_cuda.o build/temp.linux-x86_64-3.8/channelnorm_kernel.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/channelnorm_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.8/channelnorm_cuda.cpython-38-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for channelnorm_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/channelnorm_cuda.py to channelnorm_cuda.cpython-38.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying channelnorm_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying channelnorm_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying channelnorm_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying channelnorm_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.channelnorm_cuda.cpython-38: module references __file__\n",
            "creating dist\n",
            "creating 'dist/channelnorm_cuda-0.0.0-py3.8-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing channelnorm_cuda-0.0.0-py3.8-linux-x86_64.egg\n",
            "creating /root/.local/lib/python3.8/site-packages/channelnorm_cuda-0.0.0-py3.8-linux-x86_64.egg\n",
            "Extracting channelnorm_cuda-0.0.0-py3.8-linux-x86_64.egg to /root/.local/lib/python3.8/site-packages\n",
            "Adding channelnorm-cuda 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /root/.local/lib/python3.8/site-packages/channelnorm_cuda-0.0.0-py3.8-linux-x86_64.egg\n",
            "Processing dependencies for channelnorm-cuda==0.0.0\n",
            "Finished processing dependencies for channelnorm-cuda==0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK41Gd1vb87p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f22fe6-0739-4a09-87c1-677d353bcfb4"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convert.py\t\t  __init__.py\t    main.py\t   run-caffe2pytorch.sh\n",
            "datasets.py\t\t  install.sh\t    models.py\t   utils\n",
            "Dockerfile\t\t  launch_docker.sh  networks\n",
            "download_caffe_models.sh  LICENSE\t    README.md\n",
            "image.png\t\t  losses.py\t    run_a_pair.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjCvrcAC6izw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ca3286-db4a-4b0d-bff8-469ae91ee062"
      },
      "source": [
        "!python main.py --help"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: main.py\n",
            "       [-h]\n",
            "       [--start_epoch START_EPOCH]\n",
            "       [--total_epochs TOTAL_EPOCHS]\n",
            "       [--batch_size BATCH_SIZE]\n",
            "       [--train_n_batches TRAIN_N_BATCHES]\n",
            "       [--crop_size CROP_SIZE [CROP_SIZE ...]]\n",
            "       [--gradient_clip GRADIENT_CLIP]\n",
            "       [--schedule_lr_frequency SCHEDULE_LR_FREQUENCY]\n",
            "       [--schedule_lr_fraction SCHEDULE_LR_FRACTION]\n",
            "       [--rgb_max RGB_MAX]\n",
            "       [--number_workers NUMBER_WORKERS]\n",
            "       [--number_gpus NUMBER_GPUS]\n",
            "       [--no_cuda]\n",
            "       [--seed SEED]\n",
            "       [--name NAME]\n",
            "       [--save SAVE]\n",
            "       [--validation_frequency VALIDATION_FREQUENCY]\n",
            "       [--validation_n_batches VALIDATION_N_BATCHES]\n",
            "       [--render_validation]\n",
            "       [--inference]\n",
            "       [--inference_visualize]\n",
            "       [--inference_size INFERENCE_SIZE [INFERENCE_SIZE ...]]\n",
            "       [--inference_batch_size INFERENCE_BATCH_SIZE]\n",
            "       [--inference_n_batches INFERENCE_N_BATCHES]\n",
            "       [--save_flow]\n",
            "       [--resume PATH]\n",
            "       [--log_frequency LOG_FREQUENCY]\n",
            "       [--skip_training]\n",
            "       [--skip_validation]\n",
            "       [--fp16]\n",
            "       [--fp16_scale FP16_SCALE]\n",
            "       [--model {ChannelNorm,FlowNet2,FlowNet2C,FlowNet2CS,FlowNet2CSS,FlowNet2S,FlowNet2SD,Resample2d,tofp16,tofp32}]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help\n",
            "    show this\n",
            "    help\n",
            "    message and\n",
            "    exit\n",
            "  --start_epoch START_EPOCH\n",
            "  --total_epochs TOTAL_EPOCHS\n",
            "  --batch_size BATCH_SIZE, -b BATCH_SIZE\n",
            "    Batch size\n",
            "  --train_n_batches TRAIN_N_BATCHES\n",
            "    Number of\n",
            "    min-batches\n",
            "    per epoch.\n",
            "    If < 0, it\n",
            "    will be\n",
            "    determined\n",
            "    by training\n",
            "    _dataloader\n",
            "  --crop_size CROP_SIZE [CROP_SIZE ...]\n",
            "    Spatial\n",
            "    dimension\n",
            "    to crop\n",
            "    training\n",
            "    samples for\n",
            "    training\n",
            "  --gradient_clip GRADIENT_CLIP\n",
            "  --schedule_lr_frequency SCHEDULE_LR_FREQUENCY\n",
            "    in number\n",
            "    of\n",
            "    iterations\n",
            "    (0 for no\n",
            "    schedule)\n",
            "  --schedule_lr_fraction SCHEDULE_LR_FRACTION\n",
            "  --rgb_max RGB_MAX\n",
            "  --number_workers NUMBER_WORKERS, -nw NUMBER_WORKERS, --num_workers NUMBER_WORKERS\n",
            "  --number_gpus NUMBER_GPUS, -ng NUMBER_GPUS\n",
            "    number of\n",
            "    GPUs to use\n",
            "  --no_cuda\n",
            "  --seed SEED\n",
            "  --name NAME\n",
            "    a name to\n",
            "    append to\n",
            "    the save\n",
            "    directory\n",
            "  --save SAVE, -s SAVE\n",
            "    directory\n",
            "    for saving\n",
            "  --validation_frequency VALIDATION_FREQUENCY\n",
            "    validate\n",
            "    every n\n",
            "    epochs\n",
            "  --validation_n_batches VALIDATION_N_BATCHES\n",
            "  --render_validation\n",
            "    run\n",
            "    inference\n",
            "    (save flows\n",
            "    to file)\n",
            "    and every v\n",
            "    alidation_f\n",
            "    requency\n",
            "    epoch\n",
            "  --inference\n",
            "  --inference_visualize\n",
            "    visualize\n",
            "    the optical\n",
            "    flow during\n",
            "    inference\n",
            "  --inference_size INFERENCE_SIZE [INFERENCE_SIZE ...]\n",
            "    spatial\n",
            "    size\n",
            "    divisible\n",
            "    by 64.\n",
            "    default\n",
            "    (-1,-1) -\n",
            "    largest\n",
            "    possible\n",
            "    valid size\n",
            "    would be\n",
            "    used\n",
            "  --inference_batch_size INFERENCE_BATCH_SIZE\n",
            "  --inference_n_batches INFERENCE_N_BATCHES\n",
            "  --save_flow\n",
            "    save\n",
            "    predicted\n",
            "    flows to\n",
            "    file\n",
            "  --resume PATH\n",
            "    path to\n",
            "    latest\n",
            "    checkpoint\n",
            "    (default:\n",
            "    none)\n",
            "  --log_frequency LOG_FREQUENCY, --summ_iter LOG_FREQUENCY\n",
            "    Log every n\n",
            "    batches\n",
            "  --skip_training\n",
            "  --skip_validation\n",
            "  --fp16\n",
            "    Run model\n",
            "    in pseudo-\n",
            "    fp16 mode\n",
            "    (fp16\n",
            "    storage\n",
            "    fp32 math).\n",
            "  --fp16_scale FP16_SCALE\n",
            "    Loss\n",
            "    scaling,\n",
            "    positive\n",
            "    power of 2\n",
            "    values can\n",
            "    improve\n",
            "    fp16 conver\n",
            "    gence.\n",
            "\n",
            "Model:\n",
            "  --model {ChannelNorm,FlowNet2,FlowNet2C,FlowNet2CS,FlowNet2CSS,FlowNet2S,FlowNet2SD,Resample2d,tofp16,tofp32}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgzyDvlciYFs"
      },
      "source": [
        "The pretrained weights are kept in a pretrained directory and the results are kept in a result directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HKRjB0Hjxgb"
      },
      "source": [
        "!mkdir pretrained\n",
        "!mkdir results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7OkMWIiiogF"
      },
      "source": [
        "Also I had the extracted consecutive frames of the fetoscopic dataset (videoXX) gotten from https://weiss-develop.cs.ucl.ac.uk/fetoscopy-data/fetoscopy-placenta-dataset/fetoscopy-placenta-dataset.zip and kept them on my google drive. \n",
        "\n",
        "Virtually all my data was on google drive hahaha, thank you google. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkrlfoilHpDY"
      },
      "source": [
        "Adding fetoscope data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff8p9VPNDDMd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "558b9110-9b30-4dc6-92af-141456c1fc21"
      },
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "listed = drive.ListFile({'q': \"title contains 'anon001.zip'\"}).GetList()\n",
        "for file in listed:\n",
        "  print('title {}, id {}'.format(file['title'], file['id']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title anon001.zip, id 1flMv1QbSrxAH5IzV4xn5g9mljrZK-0rz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJwtH20zHi0Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a76926fc-8c8f-40a6-b01b-e9bec4285f76"
      },
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_id = file['id']\n",
        "\n",
        "fname = os.path.join('/content/flownet2-pytorch', file['title'])\n",
        "print('downloading to {}'.format(fname))\n",
        "f_ = drive.CreateFile({'id': file_id})\n",
        "f_.GetContentFile(fname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading to /content/flownet2-pytorch/anon001.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da-yCZewIULP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30cb7543-461d-4e41-bf6e-b7f188a729bf"
      },
      "source": [
        "# unzip sample images\n",
        "!unzip anon001.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  anon001.zip\n",
            "   creating: anon001/\n",
            "  inflating: anon001/anon001_00831.png  \n",
            "  inflating: anon001/anon001_00832.png  \n",
            "  inflating: anon001/anon001_00833.png  \n",
            "  inflating: anon001/anon001_00834.png  \n",
            "  inflating: anon001/anon001_00835.png  \n",
            "  inflating: anon001/anon001_00836.png  \n",
            "  inflating: anon001/anon001_00837.png  \n",
            "  inflating: anon001/anon001_00838.png  \n",
            "  inflating: anon001/anon001_00839.png  \n",
            "  inflating: anon001/anon001_00840.png  \n",
            "  inflating: anon001/anon001_00841.png  \n",
            "  inflating: anon001/anon001_00842.png  \n",
            "  inflating: anon001/anon001_00843.png  \n",
            "  inflating: anon001/anon001_00844.png  \n",
            "  inflating: anon001/anon001_00845.png  \n",
            "  inflating: anon001/anon001_00846.png  \n",
            "  inflating: anon001/anon001_00847.png  \n",
            "  inflating: anon001/anon001_00848.png  \n",
            "  inflating: anon001/anon001_00849.png  \n",
            "  inflating: anon001/anon001_00850.png  \n",
            "  inflating: anon001/anon001_00851.png  \n",
            "  inflating: anon001/anon001_00852.png  \n",
            "  inflating: anon001/anon001_00853.png  \n",
            "  inflating: anon001/anon001_00854.png  \n",
            "  inflating: anon001/anon001_00855.png  \n",
            "  inflating: anon001/anon001_00856.png  \n",
            "  inflating: anon001/anon001_00857.png  \n",
            "  inflating: anon001/anon001_00858.png  \n",
            "  inflating: anon001/anon001_00859.png  \n",
            "  inflating: anon001/anon001_00860.png  \n",
            "  inflating: anon001/anon001_00861.png  \n",
            "  inflating: anon001/anon001_00862.png  \n",
            "  inflating: anon001/anon001_00863.png  \n",
            "  inflating: anon001/anon001_00864.png  \n",
            "  inflating: anon001/anon001_00865.png  \n",
            "  inflating: anon001/anon001_00866.png  \n",
            "  inflating: anon001/anon001_00867.png  \n",
            "  inflating: anon001/anon001_00868.png  \n",
            "  inflating: anon001/anon001_00869.png  \n",
            "  inflating: anon001/anon001_00870.png  \n",
            "  inflating: anon001/anon001_00871.png  \n",
            "  inflating: anon001/anon001_00872.png  \n",
            "  inflating: anon001/anon001_00873.png  \n",
            "  inflating: anon001/anon001_00874.png  \n",
            "  inflating: anon001/anon001_00875.png  \n",
            "  inflating: anon001/anon001_00876.png  \n",
            "  inflating: anon001/anon001_00877.png  \n",
            "  inflating: anon001/anon001_00878.png  \n",
            "  inflating: anon001/anon001_00879.png  \n",
            "  inflating: anon001/anon001_00880.png  \n",
            "  inflating: anon001/anon001_00881.png  \n",
            "  inflating: anon001/anon001_00882.png  \n",
            "  inflating: anon001/anon001_00883.png  \n",
            "  inflating: anon001/anon001_00884.png  \n",
            "  inflating: anon001/anon001_00885.png  \n",
            "  inflating: anon001/anon001_00886.png  \n",
            "  inflating: anon001/anon001_00887.png  \n",
            "  inflating: anon001/anon001_00888.png  \n",
            "  inflating: anon001/anon001_00889.png  \n",
            "  inflating: anon001/anon001_00890.png  \n",
            "  inflating: anon001/anon001_00891.png  \n",
            "  inflating: anon001/anon001_00892.png  \n",
            "  inflating: anon001/anon001_00893.png  \n",
            "  inflating: anon001/anon001_00894.png  \n",
            "  inflating: anon001/anon001_00895.png  \n",
            "  inflating: anon001/anon001_00896.png  \n",
            "  inflating: anon001/anon001_00897.png  \n",
            "  inflating: anon001/anon001_00898.png  \n",
            "  inflating: anon001/anon001_00899.png  \n",
            "  inflating: anon001/anon001_00900.png  \n",
            "  inflating: anon001/anon001_00901.png  \n",
            "  inflating: anon001/anon001_00902.png  \n",
            "  inflating: anon001/anon001_00903.png  \n",
            "  inflating: anon001/anon001_00904.png  \n",
            "  inflating: anon001/anon001_00905.png  \n",
            "  inflating: anon001/anon001_00906.png  \n",
            "  inflating: anon001/anon001_00907.png  \n",
            "  inflating: anon001/anon001_00908.png  \n",
            "  inflating: anon001/anon001_00909.png  \n",
            "  inflating: anon001/anon001_00910.png  \n",
            "  inflating: anon001/anon001_00911.png  \n",
            "  inflating: anon001/anon001_00912.png  \n",
            "  inflating: anon001/anon001_00913.png  \n",
            "  inflating: anon001/anon001_00914.png  \n",
            "  inflating: anon001/anon001_00915.png  \n",
            "  inflating: anon001/anon001_00916.png  \n",
            "  inflating: anon001/anon001_00917.png  \n",
            "  inflating: anon001/anon001_00918.png  \n",
            "  inflating: anon001/anon001_00919.png  \n",
            "  inflating: anon001/anon001_00920.png  \n",
            "  inflating: anon001/anon001_00921.png  \n",
            "  inflating: anon001/anon001_00922.png  \n",
            "  inflating: anon001/anon001_00923.png  \n",
            "  inflating: anon001/anon001_00924.png  \n",
            "  inflating: anon001/anon001_00925.png  \n",
            "  inflating: anon001/anon001_00926.png  \n",
            "  inflating: anon001/anon001_00927.png  \n",
            "  inflating: anon001/anon001_00928.png  \n",
            "  inflating: anon001/anon001_00929.png  \n",
            "  inflating: anon001/anon001_00930.png  \n",
            "  inflating: anon001/anon001_00931.png  \n",
            "  inflating: anon001/anon001_00932.png  \n",
            "  inflating: anon001/anon001_00933.png  \n",
            "  inflating: anon001/anon001_00934.png  \n",
            "  inflating: anon001/anon001_00935.png  \n",
            "  inflating: anon001/anon001_00936.png  \n",
            "  inflating: anon001/anon001_00937.png  \n",
            "  inflating: anon001/anon001_00938.png  \n",
            "  inflating: anon001/anon001_00939.png  \n",
            "  inflating: anon001/anon001_00940.png  \n",
            "  inflating: anon001/anon001_00941.png  \n",
            "  inflating: anon001/anon001_00942.png  \n",
            "  inflating: anon001/anon001_00943.png  \n",
            "  inflating: anon001/anon001_00944.png  \n",
            "  inflating: anon001/anon001_00945.png  \n",
            "  inflating: anon001/anon001_00946.png  \n",
            "  inflating: anon001/anon001_00947.png  \n",
            "  inflating: anon001/anon001_00948.png  \n",
            "  inflating: anon001/anon001_00949.png  \n",
            "  inflating: anon001/anon001_00950.png  \n",
            "  inflating: anon001/anon001_00951.png  \n",
            "  inflating: anon001/anon001_00952.png  \n",
            "  inflating: anon001/anon001_00953.png  \n",
            "  inflating: anon001/anon001_00954.png  \n",
            "  inflating: anon001/anon001_00955.png  \n",
            "  inflating: anon001/anon001_00956.png  \n",
            "  inflating: anon001/anon001_00957.png  \n",
            "  inflating: anon001/anon001_00958.png  \n",
            "  inflating: anon001/anon001_00959.png  \n",
            "  inflating: anon001/anon001_00960.png  \n",
            "  inflating: anon001/anon001_00961.png  \n",
            "  inflating: anon001/anon001_00962.png  \n",
            "  inflating: anon001/anon001_00963.png  \n",
            "  inflating: anon001/anon001_00964.png  \n",
            "  inflating: anon001/anon001_00965.png  \n",
            "  inflating: anon001/anon001_00966.png  \n",
            "  inflating: anon001/anon001_00967.png  \n",
            "  inflating: anon001/anon001_00968.png  \n",
            "  inflating: anon001/anon001_00969.png  \n",
            "  inflating: anon001/anon001_00970.png  \n",
            "  inflating: anon001/anon001_00971.png  \n",
            "  inflating: anon001/anon001_00972.png  \n",
            "  inflating: anon001/anon001_00973.png  \n",
            "  inflating: anon001/anon001_00974.png  \n",
            "  inflating: anon001/anon001_00975.png  \n",
            "  inflating: anon001/anon001_00976.png  \n",
            "  inflating: anon001/anon001_00977.png  \n",
            "  inflating: anon001/anon001_00978.png  \n",
            "  inflating: anon001/anon001_00979.png  \n",
            "  inflating: anon001/anon001_00980.png  \n",
            "  inflating: anon001/anon001_00981.png  \n",
            "  inflating: anon001/anon001_00982.png  \n",
            "  inflating: anon001/anon001_00983.png  \n",
            "  inflating: anon001/anon001_00984.png  \n",
            "  inflating: anon001/anon001_00985.png  \n",
            "  inflating: anon001/anon001_00986.png  \n",
            "  inflating: anon001/anon001_00987.png  \n",
            "  inflating: anon001/anon001_00988.png  \n",
            "  inflating: anon001/anon001_00989.png  \n",
            "  inflating: anon001/anon001_00990.png  \n",
            "  inflating: anon001/anon001_00991.png  \n",
            "  inflating: anon001/anon001_00992.png  \n",
            "  inflating: anon001/anon001_00993.png  \n",
            "  inflating: anon001/anon001_00994.png  \n",
            "  inflating: anon001/anon001_00995.png  \n",
            "  inflating: anon001/anon001_00996.png  \n",
            "  inflating: anon001/anon001_00997.png  \n",
            "  inflating: anon001/anon001_00998.png  \n",
            "  inflating: anon001/anon001_00999.png  \n",
            "  inflating: anon001/anon001_01000.png  \n",
            "  inflating: anon001/anon001_01001.png  \n",
            "  inflating: anon001/anon001_01002.png  \n",
            "  inflating: anon001/anon001_01003.png  \n",
            "  inflating: anon001/anon001_01004.png  \n",
            "  inflating: anon001/anon001_01005.png  \n",
            "  inflating: anon001/anon001_01006.png  \n",
            "  inflating: anon001/anon001_01007.png  \n",
            "  inflating: anon001/anon001_01008.png  \n",
            "  inflating: anon001/anon001_01009.png  \n",
            "  inflating: anon001/anon001_01010.png  \n",
            "  inflating: anon001/anon001_01011.png  \n",
            "  inflating: anon001/anon001_01012.png  \n",
            "  inflating: anon001/anon001_01013.png  \n",
            "  inflating: anon001/anon001_01014.png  \n",
            "  inflating: anon001/anon001_01015.png  \n",
            "  inflating: anon001/anon001_01016.png  \n",
            "  inflating: anon001/anon001_01017.png  \n",
            "  inflating: anon001/anon001_01018.png  \n",
            "  inflating: anon001/anon001_01019.png  \n",
            "  inflating: anon001/anon001_01020.png  \n",
            "  inflating: anon001/anon001_01021.png  \n",
            "  inflating: anon001/anon001_01022.png  \n",
            "  inflating: anon001/anon001_01023.png  \n",
            "  inflating: anon001/anon001_01024.png  \n",
            "  inflating: anon001/anon001_01025.png  \n",
            "  inflating: anon001/anon001_01026.png  \n",
            "  inflating: anon001/anon001_01027.png  \n",
            "  inflating: anon001/anon001_01028.png  \n",
            "  inflating: anon001/anon001_01029.png  \n",
            "  inflating: anon001/anon001_01030.png  \n",
            "  inflating: anon001/anon001_01031.png  \n",
            "  inflating: anon001/anon001_01032.png  \n",
            "  inflating: anon001/anon001_01033.png  \n",
            "  inflating: anon001/anon001_01034.png  \n",
            "  inflating: anon001/anon001_01035.png  \n",
            "  inflating: anon001/anon001_01036.png  \n",
            "  inflating: anon001/anon001_01037.png  \n",
            "  inflating: anon001/anon001_01038.png  \n",
            "  inflating: anon001/anon001_01039.png  \n",
            "  inflating: anon001/anon001_01040.png  \n",
            "  inflating: anon001/anon001_01041.png  \n",
            "  inflating: anon001/anon001_01042.png  \n",
            "  inflating: anon001/anon001_01043.png  \n",
            "  inflating: anon001/anon001_01044.png  \n",
            "  inflating: anon001/anon001_01045.png  \n",
            "  inflating: anon001/anon001_01046.png  \n",
            "  inflating: anon001/anon001_01047.png  \n",
            "  inflating: anon001/anon001_01048.png  \n",
            "  inflating: anon001/anon001_01049.png  \n",
            "  inflating: anon001/anon001_01050.png  \n",
            "  inflating: anon001/anon001_01051.png  \n",
            "  inflating: anon001/anon001_01052.png  \n",
            "  inflating: anon001/anon001_01053.png  \n",
            "  inflating: anon001/anon001_01054.png  \n",
            "  inflating: anon001/anon001_01055.png  \n",
            "  inflating: anon001/anon001_01056.png  \n",
            "  inflating: anon001/anon001_01057.png  \n",
            "  inflating: anon001/anon001_01058.png  \n",
            "  inflating: anon001/anon001_01059.png  \n",
            "  inflating: anon001/anon001_01060.png  \n",
            "  inflating: anon001/anon001_01061.png  \n",
            "  inflating: anon001/anon001_01062.png  \n",
            "  inflating: anon001/anon001_01063.png  \n",
            "  inflating: anon001/anon001_01064.png  \n",
            "  inflating: anon001/anon001_01065.png  \n",
            "  inflating: anon001/anon001_01066.png  \n",
            "  inflating: anon001/anon001_01067.png  \n",
            "  inflating: anon001/anon001_01068.png  \n",
            "  inflating: anon001/anon001_01069.png  \n",
            "  inflating: anon001/anon001_01070.png  \n",
            "  inflating: anon001/anon001_01071.png  \n",
            "  inflating: anon001/anon001_01072.png  \n",
            "  inflating: anon001/anon001_01073.png  \n",
            "  inflating: anon001/anon001_01074.png  \n",
            "  inflating: anon001/anon001_01075.png  \n",
            "  inflating: anon001/anon001_01076.png  \n",
            "  inflating: anon001/anon001_01077.png  \n",
            "  inflating: anon001/anon001_01078.png  \n",
            "  inflating: anon001/anon001_01079.png  \n",
            "  inflating: anon001/anon001_01080.png  \n",
            "  inflating: anon001/anon001_01081.png  \n",
            "  inflating: anon001/anon001_01082.png  \n",
            "  inflating: anon001/anon001_01083.png  \n",
            "  inflating: anon001/anon001_01084.png  \n",
            "  inflating: anon001/anon001_01085.png  \n",
            "  inflating: anon001/anon001_01086.png  \n",
            "  inflating: anon001/anon001_01087.png  \n",
            "  inflating: anon001/anon001_01088.png  \n",
            "  inflating: anon001/anon001_01089.png  \n",
            "  inflating: anon001/anon001_01090.png  \n",
            "  inflating: anon001/anon001_01091.png  \n",
            "  inflating: anon001/anon001_01092.png  \n",
            "  inflating: anon001/anon001_01093.png  \n",
            "  inflating: anon001/anon001_01094.png  \n",
            "  inflating: anon001/anon001_01095.png  \n",
            "  inflating: anon001/anon001_01096.png  \n",
            "  inflating: anon001/anon001_01097.png  \n",
            "  inflating: anon001/anon001_01098.png  \n",
            "  inflating: anon001/anon001_01099.png  \n",
            "  inflating: anon001/anon001_01100.png  \n",
            "  inflating: anon001/anon001_01101.png  \n",
            "  inflating: anon001/anon001_01102.png  \n",
            "  inflating: anon001/anon001_01103.png  \n",
            "  inflating: anon001/anon001_01104.png  \n",
            "  inflating: anon001/anon001_01105.png  \n",
            "  inflating: anon001/anon001_01106.png  \n",
            "  inflating: anon001/anon001_01107.png  \n",
            "  inflating: anon001/anon001_01108.png  \n",
            "  inflating: anon001/anon001_01109.png  \n",
            "  inflating: anon001/anon001_01110.png  \n",
            "  inflating: anon001/anon001_01111.png  \n",
            "  inflating: anon001/anon001_01112.png  \n",
            "  inflating: anon001/anon001_01113.png  \n",
            "  inflating: anon001/anon001_01114.png  \n",
            "  inflating: anon001/anon001_01115.png  \n",
            "  inflating: anon001/anon001_01116.png  \n",
            "  inflating: anon001/anon001_01117.png  \n",
            "  inflating: anon001/anon001_01118.png  \n",
            "  inflating: anon001/anon001_01119.png  \n",
            "  inflating: anon001/anon001_01120.png  \n",
            "  inflating: anon001/anon001_01121.png  \n",
            "  inflating: anon001/anon001_01122.png  \n",
            "  inflating: anon001/anon001_01123.png  \n",
            "  inflating: anon001/anon001_01124.png  \n",
            "  inflating: anon001/anon001_01125.png  \n",
            "  inflating: anon001/anon001_01126.png  \n",
            "  inflating: anon001/anon001_01127.png  \n",
            "  inflating: anon001/anon001_01128.png  \n",
            "  inflating: anon001/anon001_01129.png  \n",
            "  inflating: anon001/anon001_01130.png  \n",
            "  inflating: anon001/anon001_01131.png  \n",
            "  inflating: anon001/anon001_01132.png  \n",
            "  inflating: anon001/anon001_01133.png  \n",
            "  inflating: anon001/anon001_01134.png  \n",
            "  inflating: anon001/anon001_01135.png  \n",
            "  inflating: anon001/anon001_01136.png  \n",
            "  inflating: anon001/anon001_01137.png  \n",
            "  inflating: anon001/anon001_01138.png  \n",
            "  inflating: anon001/anon001_01139.png  \n",
            "  inflating: anon001/anon001_01140.png  \n",
            "  inflating: anon001/anon001_01141.png  \n",
            "  inflating: anon001/anon001_01142.png  \n",
            "  inflating: anon001/anon001_01143.png  \n",
            "  inflating: anon001/anon001_01144.png  \n",
            "  inflating: anon001/anon001_01145.png  \n",
            "  inflating: anon001/anon001_01146.png  \n",
            "  inflating: anon001/anon001_01147.png  \n",
            "  inflating: anon001/anon001_01148.png  \n",
            "  inflating: anon001/anon001_01149.png  \n",
            "  inflating: anon001/anon001_01150.png  \n",
            "  inflating: anon001/anon001_01151.png  \n",
            "  inflating: anon001/anon001_01152.png  \n",
            "  inflating: anon001/anon001_01153.png  \n",
            "  inflating: anon001/anon001_01154.png  \n",
            "  inflating: anon001/anon001_01155.png  \n",
            "  inflating: anon001/anon001_01156.png  \n",
            "  inflating: anon001/anon001_01157.png  \n",
            "  inflating: anon001/anon001_01158.png  \n",
            "  inflating: anon001/anon001_01159.png  \n",
            "  inflating: anon001/anon001_01160.png  \n",
            "  inflating: anon001/anon001_01161.png  \n",
            "  inflating: anon001/anon001_01162.png  \n",
            "  inflating: anon001/anon001_01163.png  \n",
            "  inflating: anon001/anon001_01164.png  \n",
            "  inflating: anon001/anon001_01165.png  \n",
            "  inflating: anon001/anon001_01166.png  \n",
            "  inflating: anon001/anon001_01167.png  \n",
            "  inflating: anon001/anon001_01168.png  \n",
            "  inflating: anon001/anon001_01169.png  \n",
            "  inflating: anon001/anon001_01170.png  \n",
            "  inflating: anon001/anon001_01171.png  \n",
            "  inflating: anon001/anon001_01172.png  \n",
            "  inflating: anon001/anon001_01173.png  \n",
            "  inflating: anon001/anon001_01174.png  \n",
            "  inflating: anon001/anon001_01175.png  \n",
            "  inflating: anon001/anon001_01176.png  \n",
            "  inflating: anon001/anon001_01177.png  \n",
            "  inflating: anon001/anon001_01178.png  \n",
            "  inflating: anon001/anon001_01179.png  \n",
            "  inflating: anon001/anon001_01180.png  \n",
            "  inflating: anon001/anon001_01181.png  \n",
            "  inflating: anon001/anon001_01182.png  \n",
            "  inflating: anon001/anon001_01183.png  \n",
            "  inflating: anon001/anon001_01184.png  \n",
            "  inflating: anon001/anon001_01185.png  \n",
            "  inflating: anon001/anon001_01186.png  \n",
            "  inflating: anon001/anon001_01187.png  \n",
            "  inflating: anon001/anon001_01188.png  \n",
            "  inflating: anon001/anon001_01189.png  \n",
            "  inflating: anon001/anon001_01190.png  \n",
            "  inflating: anon001/anon001_01191.png  \n",
            "  inflating: anon001/anon001_01192.png  \n",
            "  inflating: anon001/anon001_01193.png  \n",
            "  inflating: anon001/anon001_01194.png  \n",
            "  inflating: anon001/anon001_01195.png  \n",
            "  inflating: anon001/anon001_01196.png  \n",
            "  inflating: anon001/anon001_01197.png  \n",
            "  inflating: anon001/anon001_01198.png  \n",
            "  inflating: anon001/anon001_01199.png  \n",
            "  inflating: anon001/anon001_01200.png  \n",
            "  inflating: anon001/anon001_01201.png  \n",
            "  inflating: anon001/anon001_01202.png  \n",
            "  inflating: anon001/anon001_01203.png  \n",
            "  inflating: anon001/anon001_01204.png  \n",
            "  inflating: anon001/anon001_01205.png  \n",
            "  inflating: anon001/anon001_01206.png  \n",
            "  inflating: anon001/anon001_01207.png  \n",
            "  inflating: anon001/anon001_01208.png  \n",
            "  inflating: anon001/anon001_01209.png  \n",
            "  inflating: anon001/anon001_01210.png  \n",
            "  inflating: anon001/anon001_01211.png  \n",
            "  inflating: anon001/anon001_01212.png  \n",
            "  inflating: anon001/anon001_01213.png  \n",
            "  inflating: anon001/anon001_01214.png  \n",
            "  inflating: anon001/anon001_01215.png  \n",
            "  inflating: anon001/anon001_01216.png  \n",
            "  inflating: anon001/anon001_01217.png  \n",
            "  inflating: anon001/anon001_01218.png  \n",
            "  inflating: anon001/anon001_01219.png  \n",
            "  inflating: anon001/anon001_01220.png  \n",
            "  inflating: anon001/anon001_01221.png  \n",
            "  inflating: anon001/anon001_01222.png  \n",
            "  inflating: anon001/anon001_01223.png  \n",
            "  inflating: anon001/anon001_01224.png  \n",
            "  inflating: anon001/anon001_01225.png  \n",
            "  inflating: anon001/anon001_01226.png  \n",
            "  inflating: anon001/anon001_01227.png  \n",
            "  inflating: anon001/anon001_01228.png  \n",
            "  inflating: anon001/anon001_01229.png  \n",
            "  inflating: anon001/anon001_01230.png  \n",
            "  inflating: anon001/anon001_01231.png  \n",
            "  inflating: anon001/anon001_01232.png  \n",
            "  inflating: anon001/anon001_01233.png  \n",
            "  inflating: anon001/anon001_01234.png  \n",
            "  inflating: anon001/anon001_01235.png  \n",
            "  inflating: anon001/anon001_01236.png  \n",
            "  inflating: anon001/anon001_01237.png  \n",
            "  inflating: anon001/anon001_01238.png  \n",
            "  inflating: anon001/anon001_01239.png  \n",
            "  inflating: anon001/anon001_01240.png  \n",
            "  inflating: anon001/anon001_01241.png  \n",
            "  inflating: anon001/anon001_01242.png  \n",
            "  inflating: anon001/anon001_01243.png  \n",
            "  inflating: anon001/anon001_01244.png  \n",
            "  inflating: anon001/anon001_01245.png  \n",
            "  inflating: anon001/anon001_01246.png  \n",
            "  inflating: anon001/anon001_01247.png  \n",
            "  inflating: anon001/anon001_01248.png  \n",
            "  inflating: anon001/anon001_01249.png  \n",
            "  inflating: anon001/anon001_01250.png  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UdFvHWsvtBr"
      },
      "source": [
        "**resize images to 448x448**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwP47UoZ8Rug"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-ipYgBbJvaz",
        "outputId": "02d15c2a-b3a1-46b3-b195-a8897a996772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXTUMEkT2z8W"
      },
      "source": [
        "IMG_SHAPE_STANDARD = (448,448);\n",
        "# frames_path = \"/content/flownet2-pytorch/anon001\"\n",
        "frames_path = \"/content/drive/MyDrive/Y4S1_UCL/Fetoscopy Placenta Dataset/Vessel_registration_unannotated_clips/video01\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dplycGstVrVM"
      },
      "source": [
        "Note: the pretrained flownet models can be gotten from the github account. Just clicking on them links them to your gdrive, or one can save it and use it later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAirEAK8I7kp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f6de7d-85d9-49b2-bf86-486edc7a84f6"
      },
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# List .txt files in the root.\n",
        "#\n",
        "# Search query reference:\n",
        "# https://developers.google.com/drive/v2/web/search-parameters\n",
        "listed = drive.ListFile({'q': \"title contains 'FlowNet2_checkpoint.pth.tar'\"}).GetList()\n",
        "for file in listed:\n",
        "  print('title {}, id {}'.format(file['title'], file['id']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title FlowNet2_checkpoint.pth.tar, id 1hF8vS6YeHkx3j2pfCeQqqZGwA_PJq_Da\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY6UZDvz_fxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef08b2e-bbea-4cd2-b076-90fdd6afd0d5"
      },
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_id = '1hF8vS6YeHkx3j2pfCeQqqZGwA_PJq_Da' \n",
        "\n",
        "fname = os.path.join('/content/flownet2-pytorch/pretrained', 'FlowNet2_checkpoint.pth.tar')\n",
        "print('downloading to {}'.format(fname))\n",
        "f_ = drive.CreateFile({'id': file_id})\n",
        "f_.GetContentFile(fname)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading to /content/flownet2-pytorch/pretrained/FlowNet2_checkpoint.pth.tar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu-_SrLlkHtI"
      },
      "source": [
        "Perform evaluation with the pretrained models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt= \"/content/drive/MyDrive/UCL_optical/FlowNet2_checkpoint.pth.tar\""
      ],
      "metadata": {
        "id": "aF19exwm5Bbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLjyqcnGX_iV",
        "outputId": "ffe0b548-6c7e-4578-eee0-65cd19ed0835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: main.py\n",
            "       [-h]\n",
            "       [--start_epoch START_EPOCH]\n",
            "       [--total_epochs TOTAL_EPOCHS]\n",
            "       [--batch_size BATCH_SIZE]\n",
            "       [--train_n_batches TRAIN_N_BATCHES]\n",
            "       [--crop_size CROP_SIZE [CROP_SIZE ...]]\n",
            "       [--gradient_clip GRADIENT_CLIP]\n",
            "       [--schedule_lr_frequency SCHEDULE_LR_FREQUENCY]\n",
            "       [--schedule_lr_fraction SCHEDULE_LR_FRACTION]\n",
            "       [--rgb_max RGB_MAX]\n",
            "       [--number_workers NUMBER_WORKERS]\n",
            "       [--number_gpus NUMBER_GPUS]\n",
            "       [--no_cuda]\n",
            "       [--seed SEED]\n",
            "       [--name NAME]\n",
            "       [--save SAVE]\n",
            "       [--validation_frequency VALIDATION_FREQUENCY]\n",
            "       [--validation_n_batches VALIDATION_N_BATCHES]\n",
            "       [--render_validation]\n",
            "       [--inference]\n",
            "       [--inference_visualize]\n",
            "       [--inference_size INFERENCE_SIZE [INFERENCE_SIZE ...]]\n",
            "       [--inference_batch_size INFERENCE_BATCH_SIZE]\n",
            "       [--inference_n_batches INFERENCE_N_BATCHES]\n",
            "       [--save_flow]\n",
            "       [--resume PATH]\n",
            "       [--log_frequency LOG_FREQUENCY]\n",
            "       [--skip_training]\n",
            "       [--skip_validation]\n",
            "       [--fp16]\n",
            "       [--fp16_scale FP16_SCALE]\n",
            "       [--model {ChannelNorm,FlowNet2,FlowNet2C,FlowNet2CS,FlowNet2CSS,FlowNet2S,FlowNet2SD,Resample2d,tofp16,tofp32}]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help\n",
            "    show this\n",
            "    help\n",
            "    message and\n",
            "    exit\n",
            "  --start_epoch START_EPOCH\n",
            "  --total_epochs TOTAL_EPOCHS\n",
            "  --batch_size BATCH_SIZE, -b BATCH_SIZE\n",
            "    Batch size\n",
            "  --train_n_batches TRAIN_N_BATCHES\n",
            "    Number of\n",
            "    min-batches\n",
            "    per epoch.\n",
            "    If < 0, it\n",
            "    will be\n",
            "    determined\n",
            "    by training\n",
            "    _dataloader\n",
            "  --crop_size CROP_SIZE [CROP_SIZE ...]\n",
            "    Spatial\n",
            "    dimension\n",
            "    to crop\n",
            "    training\n",
            "    samples for\n",
            "    training\n",
            "  --gradient_clip GRADIENT_CLIP\n",
            "  --schedule_lr_frequency SCHEDULE_LR_FREQUENCY\n",
            "    in number\n",
            "    of\n",
            "    iterations\n",
            "    (0 for no\n",
            "    schedule)\n",
            "  --schedule_lr_fraction SCHEDULE_LR_FRACTION\n",
            "  --rgb_max RGB_MAX\n",
            "  --number_workers NUMBER_WORKERS, -nw NUMBER_WORKERS, --num_workers NUMBER_WORKERS\n",
            "  --number_gpus NUMBER_GPUS, -ng NUMBER_GPUS\n",
            "    number of\n",
            "    GPUs to use\n",
            "  --no_cuda\n",
            "  --seed SEED\n",
            "  --name NAME\n",
            "    a name to\n",
            "    append to\n",
            "    the save\n",
            "    directory\n",
            "  --save SAVE, -s SAVE\n",
            "    directory\n",
            "    for saving\n",
            "  --validation_frequency VALIDATION_FREQUENCY\n",
            "    validate\n",
            "    every n\n",
            "    epochs\n",
            "  --validation_n_batches VALIDATION_N_BATCHES\n",
            "  --render_validation\n",
            "    run\n",
            "    inference\n",
            "    (save flows\n",
            "    to file)\n",
            "    and every v\n",
            "    alidation_f\n",
            "    requency\n",
            "    epoch\n",
            "  --inference\n",
            "  --inference_visualize\n",
            "    visualize\n",
            "    the optical\n",
            "    flow during\n",
            "    inference\n",
            "  --inference_size INFERENCE_SIZE [INFERENCE_SIZE ...]\n",
            "    spatial\n",
            "    size\n",
            "    divisible\n",
            "    by 64.\n",
            "    default\n",
            "    (-1,-1) -\n",
            "    largest\n",
            "    possible\n",
            "    valid size\n",
            "    would be\n",
            "    used\n",
            "  --inference_batch_size INFERENCE_BATCH_SIZE\n",
            "  --inference_n_batches INFERENCE_N_BATCHES\n",
            "  --save_flow\n",
            "    save\n",
            "    predicted\n",
            "    flows to\n",
            "    file\n",
            "  --resume PATH\n",
            "    path to\n",
            "    latest\n",
            "    checkpoint\n",
            "    (default:\n",
            "    none)\n",
            "  --log_frequency LOG_FREQUENCY, --summ_iter LOG_FREQUENCY\n",
            "    Log every n\n",
            "    batches\n",
            "  --skip_training\n",
            "  --skip_validation\n",
            "  --fp16\n",
            "    Run model\n",
            "    in pseudo-\n",
            "    fp16 mode\n",
            "    (fp16\n",
            "    storage\n",
            "    fp32 math).\n",
            "  --fp16_scale FP16_SCALE\n",
            "    Loss\n",
            "    scaling,\n",
            "    positive\n",
            "    power of 2\n",
            "    values can\n",
            "    improve\n",
            "    fp16 conver\n",
            "    gence.\n",
            "\n",
            "Model:\n",
            "  --model {ChannelNorm,FlowNet2,FlowNet2C,FlowNet2CS,FlowNet2CSS,FlowNet2S,FlowNet2SD,Resample2d,tofp16,tofp32}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/flownet2-pytorch\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x07kjC-hYpbI",
        "outputId": "e83e014a-7eaa-432e-ef00-881e10a46574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/flownet2-pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzU6ZD2fcJIj",
        "outputId": "582b3bef-2990-4d48-8a71-cda55fc57244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run our own data\n",
        "!python main.py --inference --model FlowNet2 --save_flow \\\n",
        "--inference_dataset_root \"/content/drive/MyDrive/Y4S1_UCL/Fetoscopy Placenta Dataset/Vessel_registration_unannotated_clips/video06/images\" \\\n",
        "--resume /content/drive/MyDrive/UCL_optical/FlowNet2_checkpoint.pth.tar \\\n",
        "--save \"/content/drive/MyDrive/flownet2-pytorch/resultsvideo6\" \\\n",
        "--inference_dataset ImagesFromFolder \n"
      ],
      "metadata": {
        "id": "2GP0Hd12KMIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43019c9d-d58c-408a-fc93-c86e26112466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****This is the prob:  model\n",
            "*****This is the prob:  loss\n",
            "*****This is the prob:  optimizer\n",
            "*****This is the prob:  training_dataset\n",
            "*****This is the prob:  validation_dataset\n",
            "*****This is the prob:  inference_dataset\n",
            "Parsing Arguments\n",
            "  [0.517s] \u001b[0mbatch_size: 8\u001b[0m\n",
            "  [0.517s] \u001b[0mcrop_size: [256, 256]\u001b[0m\n",
            "  [0.517s] \u001b[0mfp16: False\u001b[0m\n",
            "  [0.517s] \u001b[0mfp16_scale: 1024.0\u001b[0m\n",
            "  [0.517s] \u001b[0mgradient_clip: None\u001b[0m\n",
            "  [0.517s] \u001b[35minference: True\u001b[0m\n",
            "  [0.517s] \u001b[0minference_batch_size: 1\u001b[0m\n",
            "  [0.517s] \u001b[35minference_dataset: ImagesFromFolder\u001b[0m\n",
            "  [0.517s] \u001b[0minference_dataset_iext: png\u001b[0m\n",
            "  [0.517s] \u001b[0minference_dataset_replicates: 1\u001b[0m\n",
            "  [0.517s] \u001b[35minference_dataset_root: /content/drive/MyDrive/Y4S1_UCL/Fetoscopy Placenta Dataset/Vessel_registration_unannotated_clips/video06/images\u001b[0m\n",
            "  [0.517s] \u001b[0minference_n_batches: -1\u001b[0m\n",
            "  [0.517s] \u001b[0minference_size: [-1, -1]\u001b[0m\n",
            "  [0.517s] \u001b[0minference_visualize: False\u001b[0m\n",
            "  [0.517s] \u001b[0mlog_frequency: 1\u001b[0m\n",
            "  [0.517s] \u001b[0mloss: L1Loss\u001b[0m\n",
            "  [0.517s] \u001b[0mmodel: FlowNet2\u001b[0m\n",
            "  [0.517s] \u001b[0mmodel_batchNorm: False\u001b[0m\n",
            "  [0.517s] \u001b[0mmodel_div_flow: 20.0\u001b[0m\n",
            "  [0.517s] \u001b[0mname: run\u001b[0m\n",
            "  [0.517s] \u001b[0mno_cuda: False\u001b[0m\n",
            "  [0.517s] \u001b[35mnumber_gpus: 1\u001b[0m\n",
            "  [0.517s] \u001b[0mnumber_workers: 8\u001b[0m\n",
            "  [0.517s] \u001b[0moptimizer: Adam\u001b[0m\n",
            "  [0.517s] \u001b[0moptimizer_amsgrad: False\u001b[0m\n",
            "  [0.517s] \u001b[0moptimizer_betas: (0.9, 0.999)\u001b[0m\n",
            "  [0.517s] \u001b[0moptimizer_eps: 1e-08\u001b[0m\n",
            "  [0.517s] \u001b[0moptimizer_lr: 0.001\u001b[0m\n",
            "  [0.517s] \u001b[0moptimizer_weight_decay: 0\u001b[0m\n",
            "  [0.517s] \u001b[0mrender_validation: False\u001b[0m\n",
            "  [0.517s] \u001b[35mresume: /content/drive/MyDrive/UCL_optical/FlowNet2_checkpoint.pth.tar\u001b[0m\n",
            "  [0.517s] \u001b[0mrgb_max: 255.0\u001b[0m\n",
            "  [0.517s] \u001b[35msave: /content/drive/MyDrive/flownet2-pytorch/resultsvideo6\u001b[0m\n",
            "  [0.517s] \u001b[35msave_flow: True\u001b[0m\n",
            "  [0.517s] \u001b[0mschedule_lr_fraction: 10\u001b[0m\n",
            "  [0.517s] \u001b[0mschedule_lr_frequency: 0\u001b[0m\n",
            "  [0.517s] \u001b[0mseed: 1\u001b[0m\n",
            "  [0.517s] \u001b[0mskip_training: False\u001b[0m\n",
            "  [0.517s] \u001b[0mskip_validation: False\u001b[0m\n",
            "  [0.517s] \u001b[0mstart_epoch: 1\u001b[0m\n",
            "  [0.517s] \u001b[0mtotal_epochs: 10000\u001b[0m\n",
            "  [0.517s] \u001b[0mtrain_n_batches: -1\u001b[0m\n",
            "  [0.517s] \u001b[0mtraining_dataset: MpiSintelFinal\u001b[0m\n",
            "  [0.518s] \u001b[0mtraining_dataset_replicates: 1\u001b[0m\n",
            "  [0.518s] \u001b[0mtraining_dataset_root: ./MPI-Sintel/flow/training\u001b[0m\n",
            "  [0.518s] \u001b[0mvalidation_dataset: MpiSintelClean\u001b[0m\n",
            "  [0.518s] \u001b[0mvalidation_dataset_replicates: 1\u001b[0m\n",
            "  [0.518s] \u001b[0mvalidation_dataset_root: ./MPI-Sintel/flow/training\u001b[0m\n",
            "  [0.518s] \u001b[0mvalidation_frequency: 5\u001b[0m\n",
            "  [0.518s] \u001b[0mvalidation_n_batches: -1\u001b[0m\n",
            "  [0.553s] Operation finished\n",
            "\n",
            "Source Code\n",
            "  Current Git Hash: b'2e9e010c98931bc7cef3eb063b195f1e0ab470ba'\n",
            "\n",
            "Initializing Datasets\n",
            "  [0.018s] Inference Dataset: ImagesFromFolder\n",
            "  [0.045s] Inference Input: [3, 2, 448, 448]\n",
            "  [0.070s] Inference Targets: [3, 2, 448, 448]\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "  [0.071s] Operation finished\n",
            "\n",
            "Building FlowNet2 model\n",
            "  [3.461s] Effective Batch Size: 8\n",
            "  [3.462s] Number of parameters: 162518834\n",
            "  [3.462s] Initializing CUDA\n",
            "  [5.242s] Parallelizing\n",
            "  [5.245s] Loading checkpoint '/content/drive/MyDrive/UCL_optical/FlowNet2_checkpoint.pth.tar'\n",
            "  [5.786s] Loaded checkpoint '/content/drive/MyDrive/UCL_optical/FlowNet2_checkpoint.pth.tar' (at epoch 0)\n",
            "  [5.786s] Initializing save directory: /content/drive/MyDrive/flownet2-pytorch/resultsvideo6\n",
            "  [5.791s] Operation finished\n",
            "\n",
            "Initializing Adam Optimizer\n",
            "  [0.001s] amsgrad = False (<class 'bool'>)\n",
            "  [0.001s] weight_decay = 0 (<class 'int'>)\n",
            "  [0.001s] eps = 1e-08 (<class 'float'>)\n",
            "  [0.001s] betas = (0.9, 0.999) (<class 'tuple'>)\n",
            "  [0.001s] lr = 0.001 (<class 'float'>)\n",
            "  [0.001s] Operation finished\n",
            "\n",
            "Overall Progress:   0%|                                                       | 0/1 [00:00<?, ?it/s]\n",
            "Inferencing :   0%|                                                        | 0/99.0 [00:00<?, ?it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 9.075, EPE: 15.719:   0%|              | 0/99.0 [00:02<?, ?it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 9.075, EPE: 15.719:   1%|      | 1/99.0 [00:02<04:10,  2.55s/it]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 8.547, EPE: 14.804:   2%|      | 2/99.0 [00:02<04:07,  2.55s/it]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 8.547, EPE: 14.804:   3%|▏     | 3/99.0 [00:02<01:07,  1.42it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 7.537, EPE: 13.054:   3%|▏     | 3/99.0 [00:02<01:07,  1.42it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 7.537, EPE: 13.054:   6%|▎     | 6/99.0 [00:02<00:27,  3.40it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 7.277, EPE: 12.604:   6%|▎     | 6/99.0 [00:02<00:27,  3.40it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 7.082, EPE: 12.267:   7%|▍     | 7/99.0 [00:02<00:27,  3.40it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 7.082, EPE: 12.267:   8%|▍     | 8/99.0 [00:02<00:19,  4.61it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 7.002, EPE: 12.128:   8%|▍     | 8/99.0 [00:03<00:19,  4.61it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.928, EPE: 11.999:   9%|▌     | 9/99.0 [00:03<00:19,  4.61it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.928, EPE: 11.999:  10%|▌    | 10/99.0 [00:03<00:15,  5.83it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.857, EPE: 11.877:  10%|▌    | 10/99.0 [00:03<00:15,  5.83it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.946, EPE: 12.031:  13%|▋    | 13/99.0 [00:03<00:14,  5.83it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.946, EPE: 12.031:  14%|▋    | 14/99.0 [00:03<00:09,  9.12it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 7.018, EPE: 12.156:  14%|▋    | 14/99.0 [00:03<00:09,  9.12it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 7.009, EPE: 12.141:  15%|▊    | 15/99.0 [00:03<00:09,  9.12it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 7.009, EPE: 12.141:  16%|▊    | 16/99.0 [00:03<00:08,  9.68it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.867, EPE: 11.894:  16%|▊    | 16/99.0 [00:03<00:08,  9.68it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.793, EPE: 11.765:  19%|▉    | 19/99.0 [00:03<00:08,  9.68it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.793, EPE: 11.765:  20%|█    | 20/99.0 [00:03<00:06, 12.97it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.707, EPE: 11.617:  20%|█    | 20/99.0 [00:03<00:06, 12.97it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.613, EPE: 11.454:  21%|█    | 21/99.0 [00:03<00:06, 12.97it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.613, EPE: 11.454:  22%|█    | 22/99.0 [00:03<00:06, 12.58it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.696, EPE: 11.598:  22%|█    | 22/99.0 [00:03<00:06, 12.58it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.689, EPE: 11.586:  23%|█▏   | 23/99.0 [00:03<00:06, 12.58it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.689, EPE: 11.586:  24%|█▏   | 24/99.0 [00:03<00:06, 12.36it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.602, EPE: 11.435:  24%|█▏   | 24/99.0 [00:04<00:06, 12.36it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.512, EPE: 11.279:  27%|█▎   | 27/99.0 [00:04<00:05, 12.36it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.512, EPE: 11.279:  28%|█▍   | 28/99.0 [00:04<00:04, 15.40it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.492, EPE: 11.244:  28%|█▍   | 28/99.0 [00:04<00:04, 15.40it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.499, EPE: 11.256:  29%|█▍   | 29/99.0 [00:04<00:04, 15.40it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.499, EPE: 11.256:  30%|█▌   | 30/99.0 [00:04<00:04, 14.39it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.508, EPE: 11.272:  30%|█▌   | 30/99.0 [00:04<00:04, 14.39it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.478, EPE: 11.220:  31%|█▌   | 31/99.0 [00:04<00:04, 14.39it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.478, EPE: 11.220:  32%|█▌   | 32/99.0 [00:04<00:04, 13.67it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.479, EPE: 11.223:  32%|█▌   | 32/99.0 [00:04<00:04, 13.67it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.454, EPE: 11.179:  33%|█▋   | 33/99.0 [00:04<00:04, 13.67it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.454, EPE: 11.179:  34%|█▋   | 34/99.0 [00:04<00:04, 13.01it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.354, EPE: 11.006:  34%|█▋   | 34/99.0 [00:04<00:04, 13.01it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.257, EPE: 10.838:  37%|█▊   | 37/99.0 [00:04<00:04, 13.01it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.257, EPE: 10.838:  38%|█▉   | 38/99.0 [00:04<00:03, 16.09it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.202, EPE: 10.743:  38%|█▉   | 38/99.0 [00:04<00:03, 16.09it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.207, EPE: 10.750:  39%|█▉   | 39/99.0 [00:05<00:03, 16.09it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.207, EPE: 10.750:  40%|██   | 40/99.0 [00:05<00:03, 14.87it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.192, EPE: 10.725:  40%|██   | 40/99.0 [00:05<00:03, 14.87it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.272, EPE: 10.864:  41%|██   | 41/99.0 [00:05<00:03, 14.87it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.272, EPE: 10.864:  42%|██   | 42/99.0 [00:05<00:04, 13.95it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.272, EPE: 10.864:  42%|██   | 42/99.0 [00:05<00:04, 13.95it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.247, EPE: 10.821:  43%|██▏  | 43/99.0 [00:05<00:04, 13.95it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.247, EPE: 10.821:  44%|██▏  | 44/99.0 [00:05<00:04, 13.32it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.164, EPE: 10.676:  44%|██▏  | 44/99.0 [00:05<00:04, 13.32it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.039, EPE: 10.460:  45%|██▎  | 45/99.0 [00:05<00:04, 13.32it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 6.039, EPE: 10.460:  46%|██▎  | 46/99.0 [00:05<00:04, 12.83it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.993, EPE: 10.380:  46%|██▎  | 46/99.0 [00:05<00:04, 12.83it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.966, EPE: 10.334:  49%|██▍  | 49/99.0 [00:05<00:03, 12.83it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.966, EPE: 10.334:  51%|██▌  | 50/99.0 [00:05<00:04, 11.05it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.956, EPE: 10.316:  51%|██▌  | 50/99.0 [00:06<00:04, 11.05it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.942, EPE: 10.291:  52%|██▌  | 51/99.0 [00:06<00:04, 11.05it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.942, EPE: 10.291:  53%|██▋  | 52/99.0 [00:06<00:04, 11.22it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.941, EPE: 10.289:  53%|██▋  | 52/99.0 [00:06<00:04, 11.22it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.961, EPE: 10.324:  54%|██▋  | 53/99.0 [00:06<00:04, 11.22it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.961, EPE: 10.324:  55%|██▋  | 54/99.0 [00:06<00:03, 11.41it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.943, EPE: 10.294:  55%|██▋  | 54/99.0 [00:06<00:03, 11.41it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.945, EPE: 10.296:  56%|██▊  | 55/99.0 [00:06<00:03, 11.41it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.945, EPE: 10.296:  57%|██▊  | 56/99.0 [00:06<00:03, 11.55it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.957, EPE: 10.319:  57%|██▊  | 56/99.0 [00:06<00:03, 11.55it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.932, EPE: 10.275:  58%|██▉  | 57/99.0 [00:06<00:03, 11.55it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.932, EPE: 10.275:  59%|██▉  | 58/99.0 [00:06<00:03, 11.61it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.898, EPE: 10.215:  59%|██▉  | 58/99.0 [00:06<00:03, 11.61it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.867, EPE: 10.163:  60%|██▉  | 59/99.0 [00:06<00:03, 11.61it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.867, EPE: 10.163:  61%|███  | 60/99.0 [00:06<00:03, 11.74it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.855, EPE: 10.140:  61%|███  | 60/99.0 [00:06<00:03, 11.74it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.834, EPE: 10.104:  64%|███▏ | 63/99.0 [00:06<00:03, 11.74it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.834, EPE: 10.104:  65%|███▏ | 64/99.0 [00:06<00:02, 14.98it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.818, EPE: 10.078:  65%|███▏ | 64/99.0 [00:07<00:02, 14.98it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.785, EPE: 10.020:  66%|███▎ | 65/99.0 [00:07<00:02, 14.98it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.785, EPE: 10.020:  67%|███▎ | 66/99.0 [00:07<00:02, 13.69it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.717, EPE: 9.902:  67%|████  | 66/99.0 [00:07<00:02, 13.69it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.636, EPE: 9.762:  68%|████  | 67/99.0 [00:07<00:02, 13.69it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.636, EPE: 9.762:  69%|████  | 68/99.0 [00:07<00:02, 12.84it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.601, EPE: 9.701:  69%|████  | 68/99.0 [00:07<00:02, 12.84it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.538, EPE: 9.593:  70%|████▏ | 69/99.0 [00:07<00:02, 12.84it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.538, EPE: 9.593:  71%|████▏ | 70/99.0 [00:07<00:02, 12.48it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.453, EPE: 9.444:  71%|████▏ | 70/99.0 [00:07<00:02, 12.48it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.401, EPE: 9.354:  72%|████▎ | 71/99.0 [00:07<00:02, 12.48it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.401, EPE: 9.354:  73%|████▎ | 72/99.0 [00:07<00:02, 12.17it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.374, EPE: 9.308:  73%|████▎ | 72/99.0 [00:07<00:02, 12.17it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.352, EPE: 9.270:  74%|████▍ | 73/99.0 [00:07<00:02, 12.17it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.352, EPE: 9.270:  75%|████▍ | 74/99.0 [00:07<00:02, 11.93it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.339, EPE: 9.248:  75%|████▍ | 74/99.0 [00:07<00:02, 11.93it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.328, EPE: 9.228:  76%|████▌ | 75/99.0 [00:08<00:02, 11.93it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.328, EPE: 9.228:  77%|████▌ | 76/99.0 [00:08<00:01, 11.86it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.307, EPE: 9.192:  77%|████▌ | 76/99.0 [00:08<00:01, 11.86it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.320, EPE: 9.215:  80%|████▊ | 79/99.0 [00:08<00:01, 11.86it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.320, EPE: 9.215:  81%|████▊ | 80/99.0 [00:08<00:01, 15.33it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.374, EPE: 9.309:  81%|████▊ | 80/99.0 [00:08<00:01, 15.33it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.360, EPE: 9.284:  82%|████▉ | 81/99.0 [00:08<00:01, 15.33it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.360, EPE: 9.284:  83%|████▉ | 82/99.0 [00:08<00:01, 14.13it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.331, EPE: 9.234:  83%|████▉ | 82/99.0 [00:08<00:01, 14.13it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.297, EPE: 9.175:  84%|█████ | 83/99.0 [00:08<00:01, 14.13it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.297, EPE: 9.175:  85%|█████ | 84/99.0 [00:08<00:01, 13.33it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.266, EPE: 9.120:  85%|█████ | 84/99.0 [00:08<00:01, 13.33it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.260, EPE: 9.110:  86%|█████▏| 85/99.0 [00:08<00:01, 13.33it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.260, EPE: 9.110:  87%|█████▏| 86/99.0 [00:08<00:01, 12.83it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.228, EPE: 9.055:  87%|█████▏| 86/99.0 [00:08<00:01, 12.83it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.179, EPE: 8.971:  88%|█████▎| 87/99.0 [00:08<00:00, 12.83it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.179, EPE: 8.971:  89%|█████▎| 88/99.0 [00:08<00:00, 12.54it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.163, EPE: 8.942:  89%|█████▎| 88/99.0 [00:08<00:00, 12.54it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.154, EPE: 8.926:  90%|█████▍| 89/99.0 [00:09<00:00, 12.54it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.154, EPE: 8.926:  91%|█████▍| 90/99.0 [00:09<00:00, 12.18it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.179, EPE: 8.971:  91%|█████▍| 90/99.0 [00:09<00:00, 12.18it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.212, EPE: 9.027:  92%|█████▌| 91/99.0 [00:09<00:00, 12.18it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.212, EPE: 9.027:  93%|█████▌| 92/99.0 [00:09<00:00, 11.66it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.254, EPE: 9.099:  93%|█████▌| 92/99.0 [00:09<00:00, 11.66it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.287, EPE: 9.157:  94%|█████▋| 93/99.0 [00:09<00:00, 11.66it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.287, EPE: 9.157:  95%|█████▋| 94/99.0 [00:09<00:00, 11.40it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.347, EPE: 9.261:  95%|█████▋| 94/99.0 [00:09<00:00, 11.40it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.419, EPE: 9.385:  98%|█████▉| 97/99.0 [00:09<00:00, 11.40it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.419, EPE: 9.385:  99%|█████▉| 98/99.0 [00:09<00:00, 14.75it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.408, EPE: 9.368:  99%|█████▉| 98/99.0 [00:09<00:00, 14.75it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.397, EPE: 9.348: 100%|██████| 99/99.0 [00:09<00:00, 14.75it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.397, EPE: 9.348: : 100it [00:09, 13.87it/s]                   \u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.395, EPE: 9.345: : 100it [00:09, 13.87it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.406, EPE: 9.364: : 101it [00:09, 13.87it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.406, EPE: 9.364: : 102it [00:09, 12.41it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.422, EPE: 9.392: : 102it [00:10, 12.41it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.428, EPE: 9.401: : 103it [00:10, 12.41it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.428, EPE: 9.401: : 104it [00:10, 12.00it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.434, EPE: 9.412: : 104it [00:10, 12.00it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.425, EPE: 9.396: : 105it [00:10, 12.00it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.425, EPE: 9.396: : 106it [00:10, 11.94it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.404, EPE: 9.361: : 106it [00:10, 11.94it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.405, EPE: 9.362: : 107it [00:10, 11.94it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.405, EPE: 9.362: : 108it [00:10, 11.88it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.410, EPE: 9.370: : 108it [00:10, 11.88it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.423, EPE: 9.393: : 109it [00:10, 11.88it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.423, EPE: 9.393: : 110it [00:10,  8.00it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.454, EPE: 9.447: : 110it [00:11,  8.00it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.481, EPE: 9.494: : 111it [00:11,  8.00it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.481, EPE: 9.494: : 112it [00:11,  8.93it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.506, EPE: 9.536: : 112it [00:11,  8.93it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.554, EPE: 9.620: : 113it [00:11,  8.93it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.554, EPE: 9.620: : 114it [00:11,  9.53it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.591, EPE: 9.684: : 114it [00:11,  9.53it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.615, EPE: 9.726: : 117it [00:11,  9.53it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.615, EPE: 9.726: : 118it [00:11, 13.07it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.601, EPE: 9.702: : 118it [00:11, 13.07it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.614, EPE: 9.724: : 119it [00:11, 13.07it/s]\u001b[A\n",
            "Inference Averages for Epoch 0: L1: 5.614, EPE: 9.724: 100%|██████| 99/99.0 [00:11<00:00,  8.38it/s]\n",
            "Overall Progress: 100%|███████████████████████████████████████████████| 1/1 [00:11<00:00, 11.82s/it]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r \"/content/drive/MyDrive/flownet2-pytorch/flow_files_anon012.zip\" \"/content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjUgEjDBiA5a",
        "outputId": "d2baf07d-fbdb-4094-943e-71fd12c43c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/ (stored 0%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000000.flo (deflated 13%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000001.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000002.flo (deflated 9%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000003.flo (deflated 9%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000004.flo (deflated 10%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000005.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000006.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000007.flo (deflated 10%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000008.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000009.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000010.flo (deflated 12%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000011.flo (deflated 9%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000012.flo (deflated 9%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000013.flo (deflated 9%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000014.flo (deflated 10%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000015.flo (deflated 10%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000016.flo (deflated 9%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000017.flo (deflated 10%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000018.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000019.flo (deflated 12%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000020.flo (deflated 13%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000021.flo (deflated 13%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000022.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000023.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000024.flo (deflated 12%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000025.flo (deflated 13%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000026.flo (deflated 14%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000027.flo (deflated 13%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000028.flo (deflated 12%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000029.flo (deflated 12%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000030.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000031.flo (deflated 12%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000032.flo (deflated 13%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000033.flo (deflated 9%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000034.flo (deflated 8%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000035.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000036.flo (deflated 13%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000037.flo (deflated 13%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000038.flo (deflated 12%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000039.flo (deflated 12%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000040.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000041.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000042.flo (deflated 13%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000043.flo (deflated 12%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000044.flo (deflated 13%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000045.flo (deflated 12%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000046.flo (deflated 12%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000047.flo (deflated 9%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000048.flo (deflated 10%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000049.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000050.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000051.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000052.flo (deflated 10%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000053.flo (deflated 10%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000054.flo (deflated 10%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000055.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000056.flo (deflated 13%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000057.flo (deflated 13%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000058.flo (deflated 14%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000059.flo (deflated 13%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000060.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000061.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000062.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000063.flo (deflated 10%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000064.flo (deflated 10%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000065.flo (deflated 9%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000066.flo (deflated 8%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000067.flo (deflated 9%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000068.flo (deflated 9%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000069.flo (deflated 8%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000070.flo (deflated 8%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000071.flo (deflated 9%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000072.flo (deflated 8%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000073.flo (deflated 9%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000074.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000075.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000076.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000077.flo (deflated 10%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000078.flo (deflated 9%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000079.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000080.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000081.flo (deflated 13%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000082.flo (deflated 13%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000083.flo (deflated 13%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000084.flo (deflated 13%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000085.flo (deflated 12%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000086.flo (deflated 12%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000087.flo (deflated 10%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000088.flo (deflated 12%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000089.flo (deflated 10%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000090.flo (deflated 9%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000091.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000092.flo (deflated 10%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000093.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000094.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000095.flo (deflated 11%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000096.flo (deflated 9%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000097.flo (deflated 8%)\n",
            "  adding: content/drive/MyDrive/flownet2-pytorch/resultsvideo6/inference/run.epoch-0-flow-field/000098.flo (deflated 12%)\n"
          ]
        }
      ]
    }
  ]
}